{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1gu8CAZtDRzpPoALwKXLqjv3XNb1RsXYz","authorship_tag":"ABX9TyOS30ZCcEDfNPWt44tbYADW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TN03okxFptUo","executionInfo":{"status":"ok","timestamp":1726642356970,"user_tz":-345,"elapsed":14849,"user":{"displayName":"Utasav Kumar Yadav","userId":"01953829533474384299"}},"outputId":"774fca76-2c14-4415-d6c3-10216bbb6d7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["#Function to tokenize the text\n","def tokenize(text):\n","  return text.lower().split()"],"metadata":{"id":"iex1kGjCqK_x","executionInfo":{"status":"ok","timestamp":1726648939782,"user_tz":-345,"elapsed":1728,"user":{"displayName":"Utasav Kumar Yadav","userId":"01953829533474384299"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["#Function to calculate term frequency (TF)\n","def tf(term, tokenized_document):\n","  return tokenized_document.count(term) / len(document)\n"],"metadata":{"id":"HyiDQqSrrkG8","executionInfo":{"status":"ok","timestamp":1726648940933,"user_tz":-345,"elapsed":3,"user":{"displayName":"Utasav Kumar Yadav","userId":"01953829533474384299"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["#function to calculate inverse document frequency(IDF)\n","def inverse_document_frequency(term, all_documents):\n","  num_docs_containing_term = sum(1 for doc in all_documents if term in doc)\n","  return math.log(len(all_documents) / (1 + num_documents_term))"],"metadata":{"id":"mOc0oJ3Frw8D","executionInfo":{"status":"ok","timestamp":1726648942426,"user_tz":-345,"elapsed":2,"user":{"displayName":"Utasav Kumar Yadav","userId":"01953829533474384299"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["#Function to compute cosine similarity between two vector\n","def cosine_similarity(vector1, vector2):\n","  dot_product = np.dot(vector1, vector2)\n","  magnitude1 = np.linalg.norm(vector1)\n","  magnitude2 = np.linalg.norm(vector2)\n","  return dot_product / (norm_vec1 * norm_vec2)"],"metadata":{"id":"U1GvTW_6K6Xk","executionInfo":{"status":"ok","timestamp":1726648943420,"user_tz":-345,"elapsed":4,"user":{"displayName":"Utasav Kumar Yadav","userId":"01953829533474384299"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["#Function to compute TF-IDF For a document\n","def compute_tfidf(document, all_documents, vocab):\n","  tfidf_vector = []\n","  for term in vocab:\n","    tf = term_frequencu(term, document)\n","    idf = inverse_document_frequency(term, all_documents)\n","    tfidf_vector.append(tf * idf)\n","  return np.array(tfidf_vector)"],"metadata":{"id":"3nm2OhKXLgpX","executionInfo":{"status":"ok","timestamp":1726648946007,"user_tz":-345,"elapsed":2,"user":{"displayName":"Utasav Kumar Yadav","userId":"01953829533474384299"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Define a simple tokenizer function (replace this with your actual tokenizer)\n","def tokenize(text):\n","    return text.lower().split()\n","\n","# Function to compute TF-IDF vectors\n","def compute_tfidf(corpus, queries):\n","    vectorizer = TfidfVectorizer(tokenizer=tokenize)\n","    doc_tfidf_matrix = vectorizer.fit_transform(corpus)\n","    query_tfidf_matrix = vectorizer.transform(queries)\n","    return doc_tfidf_matrix, query_tfidf_matrix\n","\n","# Main function\n","def main():\n","    # Directory containing the text documents\n","    directory = '/content/drive/MyDrive/Week3 Document Text Files'\n","\n","    # Reading all files from the directory\n","    docs = []\n","    filenames = []\n","    for filename in os.listdir(directory):\n","        if filename.endswith(\".txt\"):\n","            with open(os.path.join(directory, filename), \"r\", encoding='latin-1') as file: # Changed encoding to latin-1, you may need to try others like 'cp1252'\n","                content = file.read()\n","                docs.append(content)\n","                filenames.append(filename)\n","\n","    # Hardcoded queries\n","    queries = ['time', 'time universe', 'complex time universe']\n","\n","    # Compute TF-IDF vectors for documents and queries\n","    doc_tfidf_matrix, query_tfidf_matrix = compute_tfidf(docs, queries)\n","\n","    # Calculate cosine similarities\n","    cosine_similarities = cosine_similarity(query_tfidf_matrix, doc_tfidf_matrix)\n","\n","    # Write the ranked results to a text file\n","    with open(\"cosine_similarities_ranked_results.txt\", \"w\") as output_file:\n","        for i, query in enumerate(queries):\n","            output_file.write(f\"\\nRanked cosine similarities for query '{query}':\\n\")\n","\n","            # Pair document filenames with their similarity scores\n","            doc_similarity_pairs = list(zip(filenames, cosine_similarities[i]))\n","            # Sort by similarity in descending order\n","            ranked_docs = sorted(doc_similarity_pairs, key=lambda x: x[1], reverse=True)\n","\n","            # Write ranked results\n","            for rank, (filename, score) in enumerate(ranked_docs, 1):\n","                output_file.write(f\"Rank {rank}: Document {filename} - Score: {score:.4f}\\n\")\n","\n","    # Optional: print ranked results for checking\n","    for i, query in enumerate(queries):\n","        print(f\"\\nRanked cosine similarities for query '{query}':\")\n","\n","        # Pair document filenames with their similarity scores\n","        doc_similarity_pairs = list(zip(filenames, cosine_similarities[i]))\n","        # Sort by similarity in descending order\n","        ranked_docs = sorted(doc_similarity_pairs, key=lambda x: x[1], reverse=True)\n","\n","        # Print ranked results\n","        for rank, (filename, score) in enumerate(ranked_docs, 1):\n","            print(f\"Rank {rank}: Document {filename} - Score: {score:.4f}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1W77CXuZBPES","executionInfo":{"status":"ok","timestamp":1726644294733,"user_tz":-345,"elapsed":3330,"user":{"displayName":"Utasav Kumar Yadav","userId":"01953829533474384299"}},"outputId":"12b0e842-f85f-4e0b-e43f-9e7d6f61a090"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Ranked cosine similarities for query 'time':\n","Rank 1: Document Document 9.txt - Score: 0.0889\n","Rank 2: Document Document 7.txt - Score: 0.0000\n","Rank 3: Document Document 2.txt - Score: 0.0000\n","Rank 4: Document Document 8.txt - Score: 0.0000\n","Rank 5: Document Document 10.txt - Score: 0.0000\n","Rank 6: Document Document 1.txt - Score: 0.0000\n","Rank 7: Document Document 3.txt - Score: 0.0000\n","Rank 8: Document Document 5.txt - Score: 0.0000\n","Rank 9: Document Document 6.txt - Score: 0.0000\n","Rank 10: Document Document 4.txt - Score: 0.0000\n","\n","Ranked cosine similarities for query 'time universe':\n","Rank 1: Document Document 9.txt - Score: 0.0889\n","Rank 2: Document Document 7.txt - Score: 0.0000\n","Rank 3: Document Document 2.txt - Score: 0.0000\n","Rank 4: Document Document 8.txt - Score: 0.0000\n","Rank 5: Document Document 10.txt - Score: 0.0000\n","Rank 6: Document Document 1.txt - Score: 0.0000\n","Rank 7: Document Document 3.txt - Score: 0.0000\n","Rank 8: Document Document 5.txt - Score: 0.0000\n","Rank 9: Document Document 6.txt - Score: 0.0000\n","Rank 10: Document Document 4.txt - Score: 0.0000\n","\n","Ranked cosine similarities for query 'complex time universe':\n","Rank 1: Document Document 9.txt - Score: 0.0677\n","Rank 2: Document Document 2.txt - Score: 0.0640\n","Rank 3: Document Document 7.txt - Score: 0.0550\n","Rank 4: Document Document 8.txt - Score: 0.0000\n","Rank 5: Document Document 10.txt - Score: 0.0000\n","Rank 6: Document Document 1.txt - Score: 0.0000\n","Rank 7: Document Document 3.txt - Score: 0.0000\n","Rank 8: Document Document 5.txt - Score: 0.0000\n","Rank 9: Document Document 6.txt - Score: 0.0000\n","Rank 10: Document Document 4.txt - Score: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]}]}]}